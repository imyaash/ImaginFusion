<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>ImaginFusion Docs</title>
    <style>
      @import url("https://fonts.googleapis.com/css2?family=Posterama&display=swap");
      html {
        color: #1a1a1a;
        background-color: #fdfdfd;
      }
      body {
        font-family: "Posterama", sans-serif;
        margin: 0 auto;
        /* max-width: 36em; */
        padding-left: 50px;
        padding-right: 50px;
        padding-top: 50px;
        padding-bottom: 50px;
        hyphens: auto;
        overflow-wrap: break-word;
        text-rendering: optimizeLegibility;
        font-kerning: normal;
        background-color: #2b2a33;
        color: #ffcd80;
      }
      @media (max-width: 600px) {
        body {
          font-size: 1em;
          padding: 12px;
        }
        h1 {
          font-size: 1.8em;
        }
      }
      @media print {
        html {
          background-color: white;
        }
        body {
          background-color: transparent;
          color: #add8e6;
          font-size: 12pt;
        }
        p,
        h2,
        h3 {
          orphans: 3;
          widows: 3;
        }
        h2,
        h3,
        h4 {
          page-break-after: avoid;
        }
      }
      p {
        margin: 1em 0;
      }
      a {
        color: #add8e6;
      }
      a:visited {
        color: #add8e6;
      }
      img {
        max-width: 100%;
      }
      svg {
        height: auto;
        max-width: 100%;
      }
      h1,
      h2,
      h3,
      h4,
      h5,
      h6 {
        margin-top: 1.4em;
      }
      h5,
      h6 {
        font-size: 1em;
        font-style: italic;
      }
      h6 {
        font-weight: normal;
      }
      ol,
      ul {
        padding-left: 1.7em;
        margin-top: 1em;
      }
      li > ol,
      li > ul {
        margin-top: 0;
      }
      blockquote {
        margin: 1em 0 1em 1.7em;
        padding-left: 1em;
        border-left: 2px solid #e6e6e6;
        color: #606060;
      }
      code {
        font-family: Menlo, Monaco, Consolas, "Lucida Console", monospace;
        font-size: 85%;
        margin: 0;
        hyphens: manual;
      }
      pre {
        margin: 1em 0;
        overflow: auto;
      }
      pre code {
        padding: 0;
        overflow: visible;
        overflow-wrap: normal;
      }
      .sourceCode {
        background-color: transparent;
        overflow: visible;
      }
      hr {
        background-color: #1a1a1a;
        border: none;
        height: 1px;
        margin: 1em 0;
      }
      table {
        margin: 1em 0;
        border-collapse: collapse;
        width: 100%;
        overflow-x: auto;
        display: block;
        font-variant-numeric: lining-nums tabular-nums;
      }
      table caption {
        margin-bottom: 0.75em;
      }
      tbody {
        margin-top: 0.5em;
        border-top: 1px solid #1a1a1a;
        border-bottom: 1px solid #1a1a1a;
      }
      th {
        border-top: 1px solid #1a1a1a;
        padding: 0.25em 0.5em 0.25em 0.5em;
      }
      td {
        padding: 0.125em 0.5em 0.25em 0.5em;
      }
      header {
        margin-bottom: 4em;
        text-align: center;
      }
      #TOC li {
        list-style: none;
      }
      #TOC ul {
        padding-left: 1.3em;
      }
      #TOC > ul {
        padding-left: 0;
      }
      #TOC a:not(:hover) {
        text-decoration: none;
      }
      code {
        white-space: pre-wrap;
      }
      span.smallcaps {
        font-variant: small-caps;
      }
      div.columns {
        display: flex;
        gap: min(4vw, 1.5em);
      }
      div.column {
        flex: auto;
        overflow-x: auto;
      }
      div.hanging-indent {
        margin-left: 1.5em;
        text-indent: -1.5em;
      }
      /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
      ul.task-list[class] {
        list-style: none;
      }
      ul.task-list li input[type="checkbox"] {
        font-size: inherit;
        width: 0.8em;
        margin: 0 0.8em 0.2em -1.6em;
        vertical-align: middle;
      }
      .display.math {
        display: block;
        text-align: center;
        margin: 0.5rem auto;
      }
    </style>
    <!--[if lt IE 9]>
      <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <h1 id="imaginfusion-documentation">ImaginFusion Documentation</h1>
    <p>
      ImaginFusion is an application to generate 3D models based on natural
      language prompts. It is based on DreamFusion, but instead of using Imagen
      and Mip-NeRF to generate 2D priors and 3D synthesis respectively, uses
      Stable Diffusion and torch-ngp. It includes two interfaces, a CLI &amp; a
      GUI, for better accessibility. The installation and usage instructions are
      in the <a href="readme.md">readme</a> file.
    </p>
    <h2 id="table-of-content">Table of Content</h2>
    <ul>
      <li><a href="#key-external-libraries">Key External Libraries</a></li>
      <li>
        <a href="#key-internal-modulesfunctions"
          >Key Internal Modules/Functions</a
        >
        <ul>
          <li>
            <a href="#command-line-interfacegraphical-user-interface"
              >Command Line Interface/Graphical User Interface</a
            >
            <ul>
              <li><a href="#modules-used">Modules Used</a></li>
              <li><a href="#user-inputs">User Inputs</a></li>
              <li><a href="#returns">Returns</a></li>
            </ul>
          </li>
          <li>
            <a href="#args">Args</a>
            <ul>
              <li><a href="#properties">Properties</a></li>
            </ul>
          </li>
          <li>
            <a href="#pipeline">Pipeline</a>
            <ul>
              <li><a href="#modules-used-1">Modules Used</a></li>
              <li><a href="#properties-1">Properties</a></li>
              <li><a href="#methods">Methods</a></li>
            </ul>
          </li>
          <li>
            <a href="#trainer">Trainer</a>
            <ul>
              <li><a href="#modules-used-2">Modules Used</a></li>
              <li><a href="#properties-2">Properties</a></li>
              <li><a href="#methods-1">Methods</a></li>
            </ul>
          </li>
          <li>
            <a href="#dataset">Dataset</a>
            <ul>
              <li><a href="#modules-used-3">Modules Used</a></li>
              <li><a href="#properties-3">Properties</a></li>
              <li><a href="#methods-2">Methods</a></li>
            </ul>
          </li>
          <li>
            <a href="#renderer">Renderer</a>
            <ul>
              <li><a href="#modules-used-4">Modules Used</a></li>
              <li><a href="#properties-4">Properties</a></li>
              <li><a href="#methods-3">Methods</a></li>
            </ul>
          </li>
          <li>
            <a href="#nerf">NeRF</a>
            <ul>
              <li><a href="#modules-used-5">Modules Used</a></li>
              <li><a href="#properties-5">Properties</a></li>
              <li><a href="#methods-4">Methods</a></li>
            </ul>
          </li>
          <li>
            <a href="#stablediffusionmodel">StableDiffusionModel</a>
            <ul>
              <li><a href="#properties-6">Properties</a></li>
              <li><a href="#methods-6">Methods</a></li>
            </ul>
          </li>
          <li><a href="#encoder">encoder</a></li>
          <li><a href="#truncexp">TruncExp</a></li>
          <li><a href="#softplus">softplus</a></li>
          <li><a href="#meshdecimator">meshDecimator</a></li>
          <li><a href="#meshcleaner">meshCleaner</a></li>
          <li><a href="#getviewdirections">getViewDirections</a></li>
          <li><a href="#custommeshgrid">customMeshGrid</a></li>
          <li><a href="#normalise">normalise</a></li>
          <li><a href="#getrays">getRays</a></li>
          <li><a href="#getcpumem">getCPUMem</a></li>
          <li><a href="#getgpumem">getGPUMem</a></li>
          <li><a href="#circleposes">circlePoses</a></li>
          <li><a href="#randposes">randPoses</a></li>
        </ul>
      </li>
    </ul>
    <h2 id="key-external-libraries">Key External Libraries</h2>
    <ul>
      <li>
        <a href="https://github.com/NVlabs/tiny-cuda-nn"
          >Tiny CUDA Neural Network Framework</a
        >
      </li>
      <li>
        <a href="https://github.com/ashawkey/torch-ngp"
          >torch-ngp: A PyTorch implementation of instant-ngp</a
        >
      </li>
      <li>
        <a href="https://github.com/gradio-app/gradio"
          >Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild</a
        >
      </li>
      <li>
        <a href="https://github.com/sail-sg/Adan"
          >Adan: A PyTorch implementation of Adaptive Nesterov Momentum
          Algorithm for Faster Optimizing by Deep Models</a
        >
      </li>
    </ul>
    <h2 id="key-internal-modulesfunctions">Key Internal Modules/Functions</h2>
    <h3 id="command-line-interfacegraphical-user-interface">
      <a href="cli.py">Command Line Interface</a>/<a href="gui.py"
        >Graphical User Interface</a
      >
    </h3>
    <p>These are the Entry Point for ImaginFusion application.</p>
    <h4 id="modules-used">Modules Used:</h4>
    <ul>
      <li><a href="utils\args.py">Args</a></li>
      <li><a href="NeRF\pipe.py">Pipeline</a></li>
    </ul>
    <h4 id="user-inputs">User Inputs:</h4>
    <ul>
      <li>posPrompt (str): Positive prompt for ImaginFusion.</li>
      <li>workspace (str): Workspace name for saving results.</li>
      <li>sdVersion (str): Stable Diffusion version</li>
      <li>hfModelKey (str): HuggingFace model key for Stable Diffusion</li>
      <li>fp16 (bool): Use mixed precision for training.</li>
      <li>seed (int): Seed value for reproducibility.</li>
      <li>iters (int): Number of iterations.</li>
      <li>lr (float): Learning Rate.</li>
      <li>lambdaEntropy (float): Loss scale for alpha entropy.</li>
      <li>maxSteps (int): Maximum number of steps sampled per ray.</li>
      <li>h (int): Render height for NeRF training.</li>
      <li>w (int): Render width for NeRF training.</li>
      <li>datasetSizeTrain (int): Size of training dataset.</li>
      <li>datasetSizeValid (int): Size of validation dataset.</li>
      <li>datasetSizeTest (int): Size of test dataset.</li>
    </ul>
    <h4 id="returns">Returns:</h4>
    <ul>
      <li>3D Model (mesh.obj, mesh.mtl files and albedo.png)</li>
      <li>360┬░ Video</li>
    </ul>
    <h3 id="args"><a href="utils\args.py">Args</a></h3>
    <p>
      Stores &amp; manages all the parameters &amp; hyperparameters for the
      application. All the user inputs are initially passed here before the
      entire Args object is passed to the Pipeline.
    </p>
    <h4 id="properties">Properties:</h4>
    <ul>
      <li>posPrompt (str): A positive text prompt.</li>
      <li>negPrompt (str): A negative text prompt.</li>
      <li>expName (str): Experiment name.</li>
      <li>workspace (str): Workspace directory.</li>
      <li>fp16 (bool): Whether to use FP16 precision.</li>
      <li>seed (int): Random seed for reproducibility.</li>
      <li>sdVersion (str): Stable Diffusion version.</li>
      <li>hfModelKey: High-frequency model key.</li>
      <li>
        evalInterval (int): Number of training iterations between evaluations on
        the validation set.
      </li>
      <li>
        testInterval (int): Number of training iterations between testing on the
        test set.
      </li>
      <li>guidanceScale (int): Guidance scale for stable diffusion.</li>
      <li>saveMesh (bool): Whether to save the mesh.</li>
      <li>mcubesResolution (int): Resolution for extracting the mesh.</li>
      <li>decimateTarget (float): Target for mesh decimation.</li>
      <li>iters (int): Number of training iterations.</li>
      <li>lr (float): Maximum learning rate.</li>
      <li>maxSteps (int): Maximum number of steps sampled per ray.</li>
      <li>
        updateExtraInterval (int): Iteration interval to update extra status.
      </li>
      <li>latentIterRatio (float): Ratio of latent iterations.</li>
      <li>albedoIterRatio (float): Ratio of albedo iterations.</li>
      <li>minAmbientRatio (float): Minimum ambient ratio.</li>
      <li>texturelessRatio (float): Textureless ratio.</li>
      <li>
        jitterPose (bool): Adding jitter to randomly sampled camera poses.
      </li>
      <li>
        jitterCentre (float): Amount of jitter to add to sampled camera poseÔÇÖs
        center.
      </li>
      <li>
        jitterTarget (float): Amount of jitter to add to sampled camera poseÔÇÖs
        target.
      </li>
      <li>
        jitterUp (float): Amount of jitter to add to sampled camera poseÔÇÖs
        up-axis.
      </li>
      <li>
        uniformSphereRate (float): Probability of sampling camera location
        uniformly.
      </li>
      <li>gradClip (float): Clip grad for all gradients.</li>
      <li>gradClipRGB (float): Clip grad of RGB space grad.</li>
      <li>bgRadius (float): Radius of the background sphere.</li>
      <li>
        densityActivation (str): Density activation function (ÔÇ£expÔÇØ or
        ÔÇ£softplusÔÇØ).
      </li>
      <li>densityThresh (float): Threshold for density grid to be occupied.</li>
      <li>blobDensity (float): Max density for density blob.</li>
      <li>blobRadius (float): Controlling the radius for density blob.</li>
      <li>optim (str): Optimization function.</li>
      <li>w (int): Render width for training NeRF.</li>
      <li>h (int): Render height for training NeRF.</li>
      <li>
        knownViewScale (float): Multiply h/w by this for known view rendering.
      </li>
      <li>batchSize (int): Number of images to be rendered per batch.</li>
      <li>bound (int): Assume the scene is bounded in box(-bound, bound)x.</li>
      <li>
        dtGamma (float): dt_gamma (&gt;=0) for adaptive ray marching. Set to 0
        to disable, &gt;0 to accelerate rendering (but usually with worse
        quality).
      </li>
      <li>minNear (float): Minimum near distance for the camera.</li>
      <li>radiusRange (list): Training camera radius range.</li>
      <li>
        thetaRange (list): Training camera along the polar axis (up-down).
      </li>
      <li>
        phiRange (list): Training camera along the azimuth axis (left-right).
      </li>
      <li>fovyRange (list): Training camera fovy range.</li>
      <li>defaultRadius (float): Radius for the default view.</li>
      <li>defaultPolar (float): Polar for the default view.</li>
      <li>defaultAzimuth (float): Azimuth for the default view.</li>
      <li>defaultFovy (float): Fovy for the default view.</li>
      <li>
        progressiveView (bool): Progressively expand view sampling range from
        default to full.
      </li>
      <li>
        progressiveViewInitRatio (float): Initial ratio of the final range.
      </li>
      <li>
        progressiveLevel (bool): Progressively increase grid encoderÔÇÖs max
        level.
      </li>
      <li>angleOverhead (float): Overhead angle.</li>
      <li>angleFront (float): Front angle.</li>
      <li>tRange (list): Range for t values.</li>
      <li>dontOverrideTRange (bool): Whether to override t range.</li>
      <li>lambdaEntropy (float): Loss scale for alpha entropy.</li>
      <li>lambdaOpacity (float): Loss scale for alpha value.</li>
      <li>lambdaOrient (float): Loss scale for orientation.</li>
      <li>lambdaGuidance (float): Loss scale for guidance.</li>
      <li>lambdaNormal (float): Loss scale for normal map.</li>
      <li>
        lambda2dNormalSmooth (float): Loss scale for 2D normal image smoothness.
      </li>
      <li>
        lambda3dNormalSmooth (float): Loss scale for 3D normal image smoothness.
      </li>
      <li>H (int): Mesh height for validation.</li>
      <li>W (int): Mesh width for validation.</li>
      <li>datasetSizeTrain (int): Length of the training dataset.</li>
      <li>
        datasetSizeValid (int): Number of frames to render in the turntable
        video during validation.
      </li>
      <li>
        datasetSizeTest (int): Number of frames to render in the turntable video
        during test time.
      </li>
      <li>expStartIter (int): Start iteration for experiment.</li>
      <li>expEndIter (int): End iteration for experiment.</li>
      <li>writeVideo (bool): Whether to write video during testing.</li>
      <li>
        emaDecay (float): Exponential moving average decay for training NeRF.
      </li>
      <li>
        schedulerUpdateEveryStep (bool): Update scheduler every training step.
      </li>
      <li>refRadii (list): Reference radii.</li>
      <li>refPolars (list): Reference polar angles.</li>
      <li>refAzimuths (list): Reference azimuth angles.</li>
    </ul>
    <h3 id="pipeline"><a href="NeRF\pipe.py">Pipeline</a></h3>
    <p>
      The class for managing the entire training pipeline. In encompasses
      various stages of the training process, including data loading, model
      initialisation, and training.
    </p>
    <h4 id="modules-used-1">Modules Used:</h4>
    <ul>
      <li><a href="NeRF\model.py">NeRF</a></li>
      <li><a href="NeRF\data.py">Dataset</a></li>
      <li><a href="NeRF\trainer.py">Trainer</a></li>
      <li><a href="utils\functions.py">seeder</a></li>
      <li><a href="sdm\model.py">StableDiffusionModel</a></li>
    </ul>
    <h4 id="properties-1">Properties:</h4>
    <ul>
      <li>
        args (object): A configuration object containing various parameters for
        the pipeline.
      </li>
      <li>device: The computing device (CPU or GPU) used for training.</li>
    </ul>
    <h4 id="methods">Methods:</h4>
    <ul>
      <li>
        loadData: Load the dataset for training, validation and testing.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                type (str, optional): The type of dataset to load (ÔÇ£trainÔÇØ,
                ÔÇ£valÔÇØ, or ÔÇ£testÔÇØ). Defaults to ÔÇ£trainÔÇØ.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>
                DataLoader: A PyTorch DataLoader object containing the loaded
                dataset.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        InitiateNeRF: Initialises the NeRF model.
        <ul>
          <li>
            Returns:
            <ul>
              <li>NeRF: An instance of NeRf model.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        InitiateGuidance: Initialises the guidance model.
        <ul>
          <li>
            Returns:
            <ul>
              <li>StableDiffusionModel: An instance of guidance model.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        trainNeRF: Train the NeRF model.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>model (NeRF): The NeRF model to be trained.</li>
              <li>guidance (StableDiffusionModel): The guidance model.</li>
              <li>trainLoader (DataLoader): DataLoader for training data.</li>
              <li>valLoader (DataLoader): DataLoader for validation data.</li>
              <li>testLoader (DataLoader): DataLoader for testing data.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        Pipeline: Starts the training pipeline by loading data, initialising
        models and training.
      </li>
    </ul>
    <h3 id="trainer"><a href="NeRF\trainer.py">Trainer</a></h3>
    <p>
      The class for training, evaluation &amp; testing the Text-to-3D model.
    </p>
    <h4 id="modules-used-2">Modules Used:</h4>
    <ul>
      <li><a href="utils\functions.py">getCPUMem</a></li>
      <li><a href="utils\functions.py">getGPUMem</a></li>
    </ul>
    <h4 id="properties-2">Properties:</h4>
    <ul>
      <li>args (object): Arguments for training.</li>
      <li>model (nn.Module): The neural network model.</li>
      <li>guidance (object): Guidance for training.</li>
      <li>expName (str): Experiment name.</li>
      <li>criterion (nn.Module, optional): Loss function. Default is None.</li>
      <li>
        optimiser (callable, optional): Optimizer for model training. Default is
        None.
      </li>
      <li>
        lrScheduler (callable, optional): Learning rate scheduler. Default is
        None.
      </li>
      <li>
        emaDecay (float, optional): Exponential moving average decay rate.
        Default is None.
      </li>
      <li>
        metrics (list, optional): List of metrics for evaluation. Default is an
        empty list.
      </li>
      <li>
        device (str, optional): Device for training (CPU or GPU). Default is
        None.
      </li>
      <li>
        verbose (bool, optional): Whether to print verbose output. Default is
        True.
      </li>
      <li>
        fp16 (bool, optional): Whether to use mixed-precision training. Default
        is False.
      </li>
      <li>
        workspace (str, optional): Workspace directory for saving logs and
        checkpoints. Default is ÔÇ£workspaceÔÇØ.
      </li>
      <li>
        bestMode (str, optional): Best mode for selecting checkpoints (min or
        max). Default is ÔÇ£minÔÇØ.
      </li>
      <li>
        useLossAsMetric (bool, optional): Whether to use loss as a metric.
        Default is True.
      </li>
      <li>
        reportMetricAtTraining (bool, optional): Whether to report metrics
        during training. Default is False.
      </li>
      <li>
        useTensorboardX (bool, optional): Whether to use TensorboardX for
        logging. Default is True.
      </li>
      <li>
        schedulerUpdateEveryStep (bool, optional): Whether to update the
        learning rate scheduler at every step. Default is False.
      </li>
    </ul>
    <h4 id="methods-1">Methods:</h4>
    <ul>
      <li>prepareEmbeddings: Prepares text embeddings during training.</li>
      <li>
        log: Logs messages to a file.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>args: Variable-length argument list.</li>
              <li>kwargs: Arbitrary keyword arguments.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        train_step: Performs a single training step.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>data (dict): Training data.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>
                Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Predicted RGB
                and depth values, and training loss.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        post_train_step: Perform post-training step actions like gradient
        scaling and clipping.
      </li>
      <li>
        eval_step: Performs a single evaluation step.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>data (dict): Evaluation data.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>
                Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Predicted RGB
                and depth values, and evaluation loss.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        test_step: Performs a single test step.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>data (dict): Testing data.</li>
              <li>
                bgColor (torch.Tensor, optional): Background colour. Defaults to
                None.
              </li>
              <li>
                perturb (bool, optional): Whether to perturb the rendering.
                Defaults to False.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>
                Tuple[torch.Tensor, torch.Tensor, None]: Predicted RGB and depth
                values, and placeholder.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        saveMesh: Saves a 3D mesh representation of the test predictions.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                path (str, optional): Path to save the mesh. Defaults to None.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        trainOneEpoch: Performs one epoch of the training.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                loader (torch.utils.data.DataLoader): DataLoader for training
                data.
              </li>
              <li>maxEpochs (int): Maximum number of epochs.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        evaluateOneEpoch: Performs one epoch of the evaluation.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                loader (torch.utils.data.DataLoader): DataLoader for evaluation
                data.
              </li>
              <li>
                name (str, optional): Name for the evaluation. Defaults to None.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        test: Performs prediction.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                loader (torch.utils.data.DataLoader): DataLoader for testing
                data.
              </li>
              <li>
                savePath (str, optional): Path to save test results. Defaults to
                None.
              </li>
              <li>
                name (str, optional): Name for the test. Defaults to None.
              </li>
              <li>
                writeVideo (bool, optional): Whether to write test results as
                video. Defaults to True.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        train: Performs training.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                trainLoader (torch.utils.data.DataLoader): DataLoader for
                training data.
              </li>
              <li>
                validLoader (torch.utils.data.DataLoader): DataLoader for
                validation data.
              </li>
              <li>
                testLoader (torch.utils.data.DataLoader): DataLoader for testing
                data.
              </li>
              <li>maxEpochs (int): Maximum number of epochs.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        evaluate: Performs evaluation.
        <ul>
          <li>
            Input:
            <ul>
              <li>
                loader (torch.utils.data.DataLoader): DataLoader for evaluation
                data.
              </li>
              <li>
                name (str, optional): Name for the evaluation. Defaults to None.
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h3 id="dataset"><a href="NeRF\data.py">Dataset</a></h3>
    <p>
      The class for managing the data used in training and inference, including
      camera poses, directions, intrinsics, and more &amp; and creates
      DataLoader object.
    </p>
    <h4 id="modules-used-3">Modules Used:</h4>
    <ul>
      <li><a href="utils\functions.py">circlePose</a></li>
      <li><a href="utils\functions.py">randPose</a></li>
      <li><a href="utils\functions.py">getRays</a></li>
    </ul>
    <h4 id="properties-3">Properties:</h4>
    <ul>
      <li>args (object): A configuration object.</li>
      <li>device (str): The device on which to perform computation.</li>
      <li>
        type (str, optional): The datase type, either ÔÇ£trainÔÇØ or ÔÇ£allÔÇØ.
        Defaults to ÔÇ£trainÔÇØ.
      </li>
      <li>H (int, optional): Height of the image. Defaults to 256.</li>
      <li>W (int, optional): Width of the image. Defaults to 256.</li>
      <li>size (int, optional): Size of the dataset. Defaults to 100.</li>
    </ul>
    <h4 id="methods-2">Methods:</h4>
    <ul>
      <li>
        collateFn: Collate function for creating batches of data.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>idx (list): List of indices to select data for batch.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>dict: A dictionary containing batched data.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        dataLoader: Creates a DataLoader for the dataset.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                batchSize (int, optional): The batch size. Defaults to None. If
                not provided, use the default batch size from args.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>DataLoader: A DataLoader object for the dataset.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        getDefaultViewData: Get data for default view(s).
        <ul>
          <li>
            Returns:
            <ul>
              <li>dict: A dictionary containing data for default views.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h3 id="renderer"><a href="NeRF\renderer.py">Renderer</a></h3>
    <p>
      The class for rendering 3D scenes, conducting raymarching, exporting 3D
      meshes, and managing density grids.
    </p>
    <h4 id="modules-used-4">Modules Used:</h4>
    <ul>
      <li><a href="utils\mesh.py">meshDecimator</a></li>
      <li><a href="utils\mesh.py">meshCleaner</a></li>
      <li><a href="utils\functions.py">customMeshGrid</a></li>
      <li><a href="utils\functions.py">normalise</a></li>
    </ul>
    <h4 id="properties-4">Properties:</h4>
    <ul>
      <li>args (dict): Configuration arguments.</li>
      <li>bound (float): The bounding box size.</li>
      <li>cascade (int): Number of cascades.</li>
      <li>gridSize (int): Size of the 3D grid.</li>
      <li>densityT (float): Density threshold.</li>
      <li>aabb_train (torch.Tensor): Training bounding box.</li>
      <li>aabb_infer (torch.Tensor): Inference bounding box.</li>
      <li>glctx: Graphics context for rendering.</li>
      <li>density_grid (torch.Tensor): Density grid for raymarching.</li>
      <li>density_bitfield (torch.Tensor): Bitfield for density grid.</li>
      <li>meanDensity (float): Mean density value.</li>
      <li>iterDensity (int): Iteration count for density updates.</li>
    </ul>
    <h4 id="methods-3">Methods:</h4>
    <ul>
      <li>
        densityBlob: Calculate density values for given points.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>x (torch.Tensor): Input points with shape [B, N, 3].</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Density values for input points.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>forward: Placeholder.</li>
      <li>density: Placeholder.</li>
      <li>resetExtraState: Reset additional state variables.</li>
      <li>
        exportMesh: Export 3D mesh to a file.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>path (str): Path to save the exported mesh.</li>
              <li>
                resolution (int, optional): Resolution for mesh generation.
                Defaults to None.
              </li>
              <li>
                decimateT (int, optional): Decimation threshold. Defaults to -1.
              </li>
              <li>
                S (int, optional): Split size for mesh grid generation. Defaults
                to 128.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        run(raysO, raysD, lightD, ambientRatio, shading, bgColor, perturb,
        tThresh, binarise, **test): Perform raymarching and rendering.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>raysO (torch.Tensor): Ray origins with shape [B, N, 3].</li>
              <li>
                raysD (torch.Tensor): Ray directions with shape [B, N, 3].
              </li>
              <li>
                lightD (torch.Tensor, optional): Light directions. Defaults to
                None.
              </li>
              <li>
                ambientRatio (float, optional): Ambient light ratio. Defaults to
                1.0.
              </li>
              <li>
                shading (str, optional): Shadind mode. Defaults to ÔÇÿalbedoÔÇÖ.
              </li>
              <li>
                bgColor (float or torch.Tensor, optional): Background colour.
                Defaults to None.
              </li>
              <li>
                perturb (bool, optional): Enable ray perturbation. Defaults to
                False.
              </li>
              <li>
                tThresh (float, optional): Threshold of t. Defaults to 1e-4.
              </li>
              <li>
                binarise (bool, optional): Binarise the output image. Defaults
                to False.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        updateExtraState: Update additional state variables.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                decay (float, optional): Decay factor for updating the density
                grid. Defaults to 0.95.
              </li>
              <li>
                S (int, optional): Split size for grid generation. Defaults to
                128.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        render(raysO, raysD, **kwargs): Render a scene using raymarching.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>raysO (torch.Tensor): Ray origins with shape [B, N, 3].</li>
              <li>
                raysD (torch.Tensor): Ray directions with shape [B, N, 3].
              </li>
              <li>
                **kwargs: Additional arguments passed to the ÔÇ£runÔÇØ method.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>dict: Rendered image, depth &amp; weights.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h3 id="nerf"><a href="NeRF\model.py">NeRF</a></h3>
    <p>
      The main instantNGP model (torch-ngp). Inherits Renderer and extends from
      it.
    </p>
    <h4 id="modules-used-5">Modules Used:</h4>
    <ul>
      <li><a href="NeRF\renderer.py">Renderer</a></li>
      <li><a href="utils\encoder.py">encoder</a></li>
      <li><a href="utils\functions.py">normalise</a></li>
      <li><a href="utils\activator.py">truncExp</a></li>
      <li><a href="utils\activator.py">softplus</a></li>
    </ul>
    <h4 id="properties-5">Properties:</h4>
    <ul>
      <li>nLayers (int): Number of layers in the NeRF network.</li>
      <li>hiddenDim (int): Hidden dimension of the NeRF network.</li>
      <li>encoder (tcnn.Encoding): TinyCUDA neural network encoder.</li>
      <li>inDim (int): Input dimension of the encoder.</li>
      <li>sigmaNet (Network): NeRF network for predicting sigma and albedo.</li>
      <li>
        densityActivation (function): Activation function for density
        prediction.
      </li>
      <li>nLayersBG (int): Number of layers in the background network.</li>
      <li>hiddenDimBG (int): Hidden dimension of the background network.</li>
      <li>encoderBG (nn.Module): Background encoder.</li>
      <li>inDimBG (int): Input dimension of the background encoder.</li>
      <li>netBG (Network): Background network.</li>
    </ul>
    <h4 id="methods-4">Methods:</h4>
    <ul>
      <li>
        forwardC: Forward pass for NeRF color prediction.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>x (torch.Tensor): Input coordinates.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>
                Tuple[torch.Tensor, torch.Tensor]: Sigma and albedo predictions.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        normal: Compute surface normals.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>x (torch.Tensor): Input coordinates.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Surface normals.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        forward: Forward pass for NeRF rendering.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>x (torch.Tensor): Input coordinates.</li>
              <li>d (torch.Tensor): Depth values.</li>
              <li>
                l (torch.Tensor, optional): Light direction vectors. Defaults to
                None.
              </li>
              <li>ratio (int, optional): Lambertian ratio. Defaults to 1.</li>
              <li>
                shading (str, optional): Shading mode (ÔÇ£albedoÔÇØ,
                ÔÇ£normalÔÇØ, ÔÇ£texturelessÔÇØ). Defaults to ÔÇÿalbedoÔÇÖ.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>
                Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Sigma, colour,
                and normal predictions.
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        density: Predict density values.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>x (torch.Tensor): Input coordinates.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>Dict[str, torch.Tensor]: Predicted sigma and albedo.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        background: Predict background values.
        <ul>
          <li>
            Args:
            <ul>
              <li>d (torch.Tensor): Depth values.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Predicted background values.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        get_params: Get network parameters and learning rates.
        <ul>
          <li>
            Args:
            <ul>
              <li>lr (float): Learning rate.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>
                List[Dist[str, Union[nn.Parameter, float]]]: List of parameter
                dictionaries.
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h3 id="stablediffusionmodel">
      <a href="sdm\model.py">StableDiffusionModel</a>
    </h3>
    <p>
      An implementation of Stable Diffusion to generate images based on natural
      language propmts and act as guiding model for NeRF.
    </p>
    <h4 id="properties-6">Properties:</h4>
    <ul>
      <li>device (str): The device on which the model is running.</li>
      <li>
        version (str): The version of the stable diffusion model being used.
      </li>
      <li>
        modelPath (str): The path to the pretrained stable-diffusion model.
      </li>
      <li>precisionT (torch.dtype): The precision type for model tensors.</li>
      <li>
        vae (nn.Module): The Variational Autoencoder component of the model.
      </li>
      <li>tokenizer: The text tokenizer used by the model.</li>
      <li>textEncoder: The text encoder used by the model.</li>
      <li>unet: The UNet component of the model.</li>
      <li>scheduler: The diffusion scheduler used by the model.</li>
      <li>numSteps (int): The total number of diffusion steps.</li>
      <li>
        minSteps (int): The minimum number of diffusion steps in the specified
        range.
      </li>
      <li>
        maxSteps (int): The maximum number of diffusion steps in the specified
        range.
      </li>
      <li>
        alphas (torch.Tensor): The alpha values used in the diffusion process.
      </li>
    </ul>
    <h4 id="methods-5">Methods:</h4>
    <ul>
      <li>
        getTextEmbeddings: Get text embeddings for a given prompt.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>
                prompt (str): The text prompt for which embeddings are to be
                generated.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Text embeddings dor the input prompt.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        produceLatents: Generate latent vectors.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>embeddings (torch.Tensor): Text embeddings.</li>
              <li>
                h (int, optional): Height of the generated image. Defaults to
                512.
              </li>
              <li>
                w (int, optional): Width of the generated image. Defaults to
                512.
              </li>
              <li>
                numSteps (int, optional): Number of diffusion steps. Defaults to
                50.
              </li>
              <li>
                guidanceScale (float, optional): Scaling factor for guidance.
                Defaults to 7.5.
              </li>
              <li>
                latents (torch.Tensor, optional): Latent vectors. Defaults to
                None.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Generated latent vectors.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        decodeLatents: Decode latent vectors into images.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>latents (torch.Tensor): Latent vectors to be decoded.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Decoded images.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        encodeImages: Encode images into latent vectors.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>images (torch.Tensor): Images to be encoded.</li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Encoded latent vectors.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        trainStep: Perform a training step.
        <ul>
          <li>
            Inputs:
            <ul>
              <li>embeddings (torch.Tensor): Text embeddings.</li>
              <li>predRGB (torch.Tensor): Predicted RGB images.</li>
              <li>
                guidanceScale (int, optional): Scaling factor for guidance.
                Defaults to 100.
              </li>
              <li>
                asLatent (bool, optional): If True, use ÔÇ£predRGBÔÇØ as latent
                vectors. Defaults to False.
              </li>
              <li>
                gradScale (int, optional): Scaling factor for gradients.
                Defaults to 1.
              </li>
            </ul>
          </li>
          <li>
            Returns:
            <ul>
              <li>torch.Tensor: Training loss</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h3 id="encoder"><a href="utils\encoder.py">encoder</a></h3>
    <p>Function to create a FreqEncoder instance.</p>
    <h4 id="inputs">Inputs:</h4>
    <ul>
      <li>inDim (int, optional): The input dimension. Defaults to 3.</li>
      <li>
        multiRes (int, optional): The degree of multi-resolution encoding.
        Defaults to 6.
      </li>
    </ul>
    <h4 id="returns-1">Returns:</h4>
    <ul>
      <li>
        Tuple: A tuple containing the frequency encoder and its output
        dimension.
      </li>
    </ul>
    <h3 id="truncexp"><a href="utils\activator.py">TruncExp</a></h3>
    <p>
      Custom autograd Function for the truncated exponential operation. This
      function computes the exponential of input values while clamping the
      output to a maximum value of 15.
    </p>
    <h4 id="inputs-1">Inputs:</h4>
    <ul>
      <li>
        ctx (Context): A PyTorch context object to save intermediate values for
        backpropagation.
      </li>
      <li>
        x (Tensor): The input tensor to compute the truncated exponential for.
      </li>
    </ul>
    <h4 id="returns-2">Returns:</h4>
    <ul>
      <li>
        Tensor: The tensor containing the truncated exponential of the input
        values.
      </li>
    </ul>
    <h3 id="softplus"><a href="utils\activator.py">softplus</a></h3>
    <p>
      Fuction to compute biased softplus activation. The softplus function is
      defined as softplus(x) = ln(1 + exp(x)), and this implementation allows an
      optional bias to be applied before computing the softplus.
    </p>
    <h4 id="inputs-2">Inputs:</h4>
    <ul>
      <li>x (Tensor): The input tensor to apply the softplus activation to.</li>
      <li>
        bias (float): An optional bias value to be subtracted from the input
        tensor before applying softplus.
      </li>
    </ul>
    <h4 id="returns-3">Returns:</h4>
    <ul>
      <li>Tensor: The tensor containing the softplus activations.</li>
    </ul>
    <h3 id="meshdecimator"><a href="utils\mesh.py">meshDecimator</a></h3>
    <p>Function to decimate a mesh while preserving itÔÇÖs shape.</p>
    <h4 id="inputs-3">Inputs:</h4>
    <ul>
      <li>vertices (numpy.ndarray): The vertices of the input mesh.</li>
      <li>faces (numpy.ndarray): The faces of the input mesh.</li>
      <li>target (int): The target number of faces after decimation.</li>
      <li>
        remesh (bool, optional): Whether to remesh the mesh after decimation.
        Defaults to False.
      </li>
      <li>
        optimalPlacement (bool, optional): Whether to use optimal placement
        during decimatin. Defaults to True.
      </li>
    </ul>
    <h4 id="returns-4">Returns:</h4>
    <ul>
      <li>
        Tuple[numpy.ndarray, numpy.ndarray]: The vertices and faces of the
        decimated mesh.
      </li>
    </ul>
    <h3 id="meshcleaner"><a href="utils\mesh.py">meshCleaner</a></h3>
    <p>Function to clean and repair 3D mesh.</p>
    <h4 id="inputs-4">Inputs:</h4>
    <ul>
      <li>vertices (numpy.ndarray): The vertices of the input mesh.</li>
      <li>faces (numpy.ndarray): The faces of the input mesh.</li>
      <li>
        vPct (int, optional): Percentage of close vertices of merge. Defaults to
        1.
      </li>
      <li>
        minF (int, optional): Minimum number of faces in connected components to
        keep. Defaults to 8.
      </li>
      <li>
        minD (int, optional): Minimum diameter of connected components to keep.
        Defaults to 5.
      </li>
      <li>
        repair (bool, optional): Whether to repair non-manifold edges and
        vertices. Defaults to True.
      </li>
      <li>
        remesh (bool, optional): Whether to remesh the mesh after cleaning.
        Defaults to True.
      </li>
      <li>
        remeshSize (float, optional): Target edge length for remeshing. Defaults
        to 0.01.
      </li>
    </ul>
    <h4 id="returns-5">Returns:</h4>
    <ul>
      <li>
        Tuple[numpy.ndarray, numpy.ndarray]: The vertices and faces of the
        cleaned and repaired mesh.
      </li>
    </ul>
    <h3 id="getviewdirections">
      <a href="utils\functions.py">getViewDirections</a>
    </h3>
    <p>
      Function to calculate the view direction based on the angles, the thetas,
      and the phis.
    </p>
    <h4 id="inputs-5">Inputs:</h4>
    <ul>
      <li>thetas (torch.Tensor): Tensor containing theta angles in radians.</li>
      <li>phis (torch.Tensor): Tensor containing phi angles in radians.</li>
      <li>oHead (float): Angle overhead threshold in radians.</li>
      <li>front (float): Angle front threshold in radians.</li>
    </ul>
    <h4 id="returns-6">Returns:</h4>
    <ul>
      <li>torch.Tensor: A tensor of integers representing view directions.</li>
    </ul>
    <h3 id="custommeshgrid"><a href="utils\functions.py">customMeshGrid</a></h3>
    <p>Function to create a mesh grid for given input tensors.</p>
    <h4 id="inputs-6">Inputs:</h4>
    <ul>
      <li>args: Input tensors for which the mesh grid should be created.</li>
    </ul>
    <h4 id="returns-7">Returns:</h4>
    <ul>
      <li>tuple: A tuple of tensors representing the mesh grid.</li>
    </ul>
    <h3 id="normalise"><a href="utils\functions.py">normalise</a></h3>
    <p>Function to normalise a tensor.</p>
    <h4 id="inputs-7">Inputs:</h4>
    <ul>
      <li>x (torch.Tensor): Input tensor.</li>
      <li>
        eps (float, optional): A small value to prevent division by zero.
        Defaults to 1e-20.
      </li>
    </ul>
    <h4 id="returns-8">Returns:</h4>
    <ul>
      <li>torch.Tensor: normalised tensor.</li>
    </ul>
    <h3 id="getrays"><a href="utils\functions.py">getRays</a></h3>
    <p>Function to generate rays based on camera poses and intrinsics.</p>
    <h4 id="inputs-8">Inputs:</h4>
    <ul>
      <li>poses (torch.Tensor): Camera poses.</li>
      <li>intrinsics (tuple): Camera intrinsics (fx, fy, cx, cy)</li>
      <li>H (int): Image height.</li>
      <li>W (int): image width.</li>
      <li>
        N (int, optional): Number of rays to generate. Defaults to -1, generates
        all rays.
      </li>
      <li>
        errorMap (torch.Tensor, optional): Error map for ray sampling. Defaults
        to None.
      </li>
    </ul>
    <h4 id="returns-9">Returns:</h4>
    <ul>
      <li>
        dict: A dictionary containing ray information including origins,
        directions, and indices.
      </li>
    </ul>
    <h3 id="seeder"><a href="utils\functions.py">seeder</a></h3>
    <p>Function to set random seed for Python, NumPy, and PyTorch.</p>
    <h4 id="inputs-9">Inputs:</h4>
    <ul>
      <li>seed (int): Random seed value.</li>
    </ul>
    <h3 id="getcpumem"><a href="utils\functions.py">getCPUMem</a></h3>
    <p>Function to get current memory usage.</p>
    <h4 id="returns-10">Returns:</h4>
    <ul>
      <li>float: Current CPU memory usage in GB.</li>
    </ul>
    <h3 id="getgpumem"><a href="utils\functions.py">getGPUMem</a></h3>
    <p>Function to get current GPU usage.</p>
    <h4 id="returns-11">Returns:</h4>
    <ul>
      <li>
        tuple: A tuple containing the total GPU memory and GPU memory usage for
        each available GPU.
      </li>
    </ul>
    <h3 id="circleposes"><a href="utils\functions.py">circlePoses</a></h3>
    <p>Function to generate circular camera poses.</p>
    <h4 id="inputs-10">Inputs:</h4>
    <ul>
      <li>device (str): PyTorch device.</li>
      <li>
        radius (torch.Tensor, optional): Radius of the circle. Defaults to
        torch.tensor([3.2]).
      </li>
      <li>
        theta (torch.Tensor, optional): Theta angles in degrees. Defaults to
        torch.tensor([60]).
      </li>
      <li>
        phi (torch.Tensor, optional): Phi angles in degrees. Defaults to
        torch.tensor([0]).
      </li>
      <li>
        returnDirs (bool, optional): Whether to return view directions. Defaults
        to False.
      </li>
      <li>
        angleOverhead (int, optional): Angle overhead threshold in degrees.
        Defaults to 30.
      </li>
      <li>
        angleFront (int, optional): Angle front threshold in degrees. Defaults
        to 60.
      </li>
    </ul>
    <h4 id="returns-12">Returns:</h4>
    <ul>
      <li>
        tuple: A tuple containing camera poses and view directions (if
        returnDirs is True).
      </li>
    </ul>
    <h3 id="randposes"><a href="utils\functions.py">randPoses</a></h3>
    <p>Function to genarate random camera poses.</p>
    <h4 id="inputs-11">Inputs:</h4>
    <ul>
      <li>size (int): Number of camera poses to generate.</li>
      <li>device (str): PyTorch device.</li>
      <li>args (object): Additional arguments.</li>
      <li>
        radRange (list, optional): Range for the radius. Defaults to None.
      </li>
      <li>
        thetaRange (list, optional): Range for theta angles in degrees. Defaults
        to None.
      </li>
      <li>
        phiRange (list, optional): Range for phi angles in degrees. Defaults to
        None.
      </li>
      <li>
        returnDirs (bool, optional): Whether to return view directions. Defaults
        to False.
      </li>
      <li>
        angleOverhead (int, optional): Angle overhead threshold in degrees.
        Defaults to 30.
      </li>
      <li>
        angleFront (int, optional): Angle front threshold in degrees. Defaults
        to 60.
      </li>
      <li>
        uniSphRate (float, optional): Rate of uniform spherical sampling.
        Defaults to 0.5.
      </li>
    </ul>
    <h4 id="returns-13">Returns:</h4>
    <ul>
      <li>
        tuple: A tuple containing camera poses, view directions, theta angles,
        phi angles, and radii.
      </li>
    </ul>
  </body>
</html>
